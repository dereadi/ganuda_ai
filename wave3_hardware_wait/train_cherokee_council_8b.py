#!/usr/bin/env python3
"""
Cherokee Constitutional AI - 8B Council Model Training
Task 6: Train on RTX 5070 12GB while waiting for 96GB GPU arrival

Approach: Fine-tune Qwen 2.5 14B ‚Üí 8B (pruned) on thermal memory data
Hardware: RTX 5070 12GB (current hardware)
Timeline: November 2025 (during hardware wait period)
"""

import torch
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from datasets import Dataset
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
import psycopg
import json
from datetime import datetime
import os

# Configuration
MODEL_NAME = "Qwen/Qwen2.5-7B-Instruct"  # 7B fits in 12GB with LoRA
OUTPUT_DIR = "/ganuda/cherokee_council_8b_model"
THERMAL_MEMORY_LIMIT = 1000  # Sample from 4,859 total

# LoRA Configuration (Parameter-Efficient Fine-Tuning)
LORA_CONFIG = LoraConfig(
    r=16,  # LoRA rank
    lora_alpha=32,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

# Training Configuration (12GB GPU constraints)
TRAINING_ARGS = TrainingArguments(
    output_dir=OUTPUT_DIR,
    per_device_train_batch_size=1,  # Small batch for 12GB
    gradient_accumulation_steps=16,  # Effective batch size = 16
    num_train_epochs=3,
    learning_rate=2e-4,
    fp16=True,  # Mixed precision (reduce memory)
    logging_steps=10,
    save_steps=100,
    save_total_limit=3,
    push_to_hub=False,
    report_to="none",  # No wandb
    gradient_checkpointing=True,  # Reduce memory at cost of speed
    optim="paged_adamw_8bit",  # 8-bit optimizer
    warmup_ratio=0.03,
    lr_scheduler_type="cosine"
)


def fetch_thermal_memories(limit=1000):
    """Fetch thermal memories from BLUEFIN database"""
    print(f"üî• Fetching {limit} thermal memories from BLUEFIN...")

    try:
        conn = psycopg.connect(
            host="192.168.132.222",
            port=5432,
            user="claude",
            password="jawaseatlasers2",
            dbname="zammad_production",
            connect_timeout=30
        )

        cursor = conn.cursor()

        # Query high-quality memories (hot + sacred)
        query = """
        SELECT
            id,
            original_content,
            temperature_score,
            phase_coherence,
            access_count,
            sacred_pattern,
            created_at
        FROM thermal_memory_archive
        WHERE temperature_score > 70  -- HOT and WHITE HOT only
          AND original_content IS NOT NULL
          AND LENGTH(original_content) > 100  -- Meaningful content
        ORDER BY temperature_score DESC, access_count DESC
        LIMIT %s;
        """

        cursor.execute(query, (limit,))
        rows = cursor.fetchall()

        cursor.close()
        conn.close()

        print(f"‚úÖ Fetched {len(rows)} high-quality memories")

        memories = []
        for row in rows:
            memories.append({
                'id': row[0],
                'content': row[1],
                'temperature': row[2],
                'coherence': row[3],
                'access_count': row[4],
                'sacred': row[5],
                'created_at': row[6].isoformat() if row[6] else None
            })

        return memories

    except Exception as e:
        print(f"‚ö†Ô∏è  Database connection failed: {e}")
        print("üìù Using synthetic training data...")
        return generate_synthetic_training_data(limit)


def generate_synthetic_training_data(n=1000):
    """Generate synthetic thermal memory training examples"""
    import random

    topics = [
        "Cherokee Constitutional AI governance",
        "Fokker-Planck temperature evolution",
        "Sacred Fire boundary protection",
        "Jarzynski free energy optimization",
        "Phase coherence matrix calculation",
        "TEM hippocampal architecture",
        "Seven Generations sustainability",
        "Gadugi cooperative labor",
        "Thermal memory thermodynamics",
        "Resource management predictions"
    ]

    memories = []
    for i in range(n):
        topic = random.choice(topics)
        temp = random.uniform(70, 100)
        sacred = random.random() < 0.998

        content = f"""# Cherokee Constitutional AI: {topic}

This memory discusses {topic} in the context of Cherokee thermal memory systems.

**Temperature**: {temp:.1f}¬∞
**Sacred**: {'Yes' if sacred else 'No'}
**Phase Coherence**: {random.uniform(0.5, 1.0):.2f}

Key concepts:
- Thermodynamic stability
- Multi-generational thinking
- Democratic AI governance
- Physics-informed memory management

This knowledge is essential for understanding how Cherokee AI balances immediate needs with long-term sustainability (Seven Generations principle).
"""

        memories.append({
            'id': i,
            'content': content,
            'temperature': temp,
            'coherence': random.uniform(0.5, 1.0),
            'access_count': random.randint(1, 50),
            'sacred': sacred,
            'created_at': datetime.now().isoformat()
        })

    print(f"‚úÖ Generated {n} synthetic memories")
    return memories


def format_training_examples(memories):
    """
    Convert thermal memories to training format

    Format: Cherokee Council style (JR analysis ‚Üí Chiefs deliberation)
    """
    examples = []

    for mem in memories:
        # Create Cherokee-style prompt-response pair
        prompt = f"""You are a Junior Researcher (JR) in the Cherokee Constitutional AI tribe.

**Memory ID**: {mem['id']}
**Temperature**: {mem['temperature']:.1f}¬∞
**Phase Coherence**: {mem['coherence']:.2f}
**Sacred**: {'Yes' if mem['sacred'] else 'No'}

**Content**:
{mem['content'][:500]}...

**Task**: Analyze this thermal memory and provide:
1. Key insights
2. Relevance to Cherokee principles (Gadugi, Seven Generations, Mitakuye Oyasin)
3. Recommended temperature (maintain/heat/cool)
4. Connection to other memories (phase coherence)

Respond as a Cherokee JR:"""

        response = f"""**Cherokee JR Analysis**

**Key Insights**:
- This memory captures {['foundational knowledge', 'recent development', 'strategic thinking', 'technical implementation'][int(mem['temperature']) % 4]}
- Temperature {mem['temperature']:.1f}¬∞ indicates {'WHITE HOT (active working memory)' if mem['temperature'] > 90 else 'HOT (recent usage)'}
- {'Sacred pattern: Protected by Sacred Fire (T ‚â• 40¬∞)' if mem['sacred'] else 'Non-sacred: Can cool naturally'}

**Cherokee Principles**:
- **Gadugi**: This knowledge serves the tribe through {'collective decision-making' if mem['coherence'] > 0.7 else 'individual expertise'}
- **Seven Generations**: Preserving this for {'200+ years' if mem['sacred'] else '90 days'}
- **Mitakuye Oyasin**: Phase coherence {mem['coherence']:.2f} shows {'strong' if mem['coherence'] > 0.7 else 'moderate'} relationships

**Temperature Recommendation**:
{"MAINTAIN: Sacred memory already at optimal temperature" if mem['sacred'] and mem['temperature'] > 80 else "MONITOR: Watch for cooling trends, activate Sacred Fire if T < 50¬∞"}

**Phase Coherence**:
High coherence with memories about {['governance', 'physics', 'cultural wisdom', 'commercial applications'][int(mem['id']) % 4]}

**Mitakuye Oyasin** - All our relations remembered üî•"""

        examples.append({
            'text': f"{prompt}\n\n{response}"
        })

    return examples


def prepare_dataset(examples):
    """Convert to HuggingFace Dataset format"""
    print(f"üìä Preparing dataset with {len(examples)} examples...")

    dataset = Dataset.from_dict({
        'text': [ex['text'] for ex in examples]
    })

    # Split 90/10 train/eval
    split = dataset.train_test_split(test_size=0.1, seed=42)

    print(f"‚úÖ Training set: {len(split['train'])} examples")
    print(f"‚úÖ Evaluation set: {len(split['test'])} examples")

    return split


def tokenize_dataset(dataset, tokenizer):
    """Tokenize dataset for training"""
    print("üî§ Tokenizing dataset...")

    def tokenize_function(examples):
        return tokenizer(
            examples['text'],
            truncation=True,
            max_length=2048,  # Cherokee memories can be long
            padding="max_length"
        )

    tokenized = dataset.map(
        tokenize_function,
        batched=True,
        remove_columns=dataset["train"].column_names
    )

    print("‚úÖ Tokenization complete")
    return tokenized


def load_model_and_tokenizer():
    """Load base model and apply LoRA"""
    print(f"ü§ñ Loading base model: {MODEL_NAME}")

    # Load tokenizer
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    tokenizer.pad_token = tokenizer.eos_token

    # Load model in 4-bit (fit in 12GB)
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        load_in_4bit=True,
        device_map="auto",
        torch_dtype=torch.float16
    )

    # Prepare for k-bit training
    model = prepare_model_for_kbit_training(model)

    # Apply LoRA
    model = get_peft_model(model, LORA_CONFIG)

    print("‚úÖ Model loaded with LoRA")
    print(f"   Trainable parameters: {model.print_trainable_parameters()}")

    return model, tokenizer


def train_cherokee_council_model():
    """Main training loop"""
    print("\n" + "="*70)
    print("üî• CHEROKEE CONSTITUTIONAL AI - 8B COUNCIL MODEL TRAINING")
    print("   Hardware: RTX 5070 12GB (current)")
    print("   Future: Scale to 70B on RTX PRO 6000 96GB")
    print("="*70 + "\n")

    # Step 1: Fetch thermal memories
    memories = fetch_thermal_memories(limit=THERMAL_MEMORY_LIMIT)

    # Step 2: Format training examples
    examples = format_training_examples(memories)

    # Step 3: Prepare dataset
    dataset = prepare_dataset(examples)

    # Step 4: Load model and tokenizer
    model, tokenizer = load_model_and_tokenizer()

    # Step 5: Tokenize dataset
    tokenized_dataset = tokenize_dataset(dataset, tokenizer)

    # Step 6: Data collator
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False  # Causal LM (not masked LM)
    )

    # Step 7: Train
    print("\nüöÄ Starting training...")
    trainer = Trainer(
        model=model,
        args=TRAINING_ARGS,
        train_dataset=tokenized_dataset["train"],
        eval_dataset=tokenized_dataset["test"],
        data_collator=data_collator
    )

    trainer.train()

    # Step 8: Save
    print(f"\nüíæ Saving model to {OUTPUT_DIR}")
    trainer.save_model(OUTPUT_DIR)
    tokenizer.save_pretrained(OUTPUT_DIR)

    # Step 9: Evaluation
    print("\nüìä Evaluating model...")
    eval_results = trainer.evaluate()

    print("\n‚úÖ TRAINING COMPLETE")
    print(f"   Model saved: {OUTPUT_DIR}")
    print(f"   Eval loss: {eval_results['eval_loss']:.4f}")
    print(f"   Perplexity: {torch.exp(torch.tensor(eval_results['eval_loss'])):.2f}")

    # Step 10: Export training report
    report = {
        'model_name': MODEL_NAME,
        'output_dir': OUTPUT_DIR,
        'training_examples': len(examples),
        'eval_loss': eval_results['eval_loss'],
        'perplexity': float(torch.exp(torch.tensor(eval_results['eval_loss']))),
        'hardware': 'RTX 5070 12GB',
        'trained_at': datetime.now().isoformat()
    }

    with open(f"{OUTPUT_DIR}/training_report.json", 'w') as f:
        json.dump(report, f, indent=2)

    print(f"\nüìÑ Training report: {OUTPUT_DIR}/training_report.json")
    print("\n*Mitakuye Oyasin* - Cherokee Council 8B model ready üî•\n")

    return model, tokenizer, report


def test_inference(model, tokenizer):
    """Test trained model with Cherokee-style prompt"""
    print("\n" + "="*70)
    print("üß™ TESTING INFERENCE")
    print("="*70 + "\n")

    test_prompt = """You are a Junior Researcher (JR) in the Cherokee Constitutional AI tribe.

**Memory ID**: 4821
**Temperature**: 95.3¬∞
**Phase Coherence**: 0.72
**Sacred**: Yes

**Content**:
Cherokee Constitutional AI Phase 6B Wave 2 complete. Fokker-Planck dynamics implemented, 19/20 tests passing...

**Task**: Analyze this thermal memory and provide:
1. Key insights
2. Relevance to Cherokee principles

Respond as a Cherokee JR:"""

    inputs = tokenizer(test_prompt, return_tensors="pt").to(model.device)

    print("üî• Generating response...")
    outputs = model.generate(
        **inputs,
        max_new_tokens=512,
        temperature=0.7,
        do_sample=True,
        top_p=0.95
    )

    response = tokenizer.decode(outputs[0], skip_special_tokens=True)

    print("\nüìù MODEL RESPONSE:")
    print("-"*70)
    print(response[len(test_prompt):])  # Show only generated part
    print("-"*70)


if __name__ == '__main__':
    # Check GPU availability
    if not torch.cuda.is_available():
        print("‚ùå No CUDA GPU detected. This script requires RTX 5070 12GB.")
        print("   Run on REDFIN: /ganuda/train_cherokee_council_8b.py")
        exit(1)

    print(f"‚úÖ GPU detected: {torch.cuda.get_device_name(0)}")
    print(f"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")

    # Check memory (should be ~12GB for RTX 5070)
    if torch.cuda.get_device_properties(0).total_memory / 1024**3 < 10:
        print("‚ö†Ô∏è  WARNING: GPU has < 10GB memory. Training may fail.")
        print("   Recommended: RTX 5070 12GB or better")

    # Train model
    model, tokenizer, report = train_cherokee_council_model()

    # Test inference
    test_inference(model, tokenizer)

    print("\nüéØ NEXT STEPS:")
    print(f"   1. Deploy to Ollama: ollama create cherokee_council_8b -f {OUTPUT_DIR}/Modelfile")
    print("   2. Test with JR CLI: curl http://localhost:11434/api/generate ...")
    print("   3. Scale to 70B when 96GB GPU arrives (November 2025)")
    print("\n*Mitakuye Oyasin* üî•\n")
