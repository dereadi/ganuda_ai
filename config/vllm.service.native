[Unit]
Description=vLLM OpenAI-Compatible Inference Server (Native sm_120)
After=network.target

[Service]
Type=simple
User=dereadi
Group=dereadi
WorkingDirectory=/home/dereadi
Environment=PATH=/home/dereadi/cherokee_venv/bin:/usr/bin:/bin
Environment=HF_HOME=/ganuda/home/dereadi/.cache/huggingface
Environment=CUDA_HOME=/ganuda/cuda-12.8
Environment=LD_LIBRARY_PATH=/ganuda/cuda-12.8/lib64:/ganuda/cuda-12.8/lib
ExecStart=/home/dereadi/cherokee_venv/bin/python -m vllm.entrypoints.openai.api_server \
  --model /ganuda/models/qwen2.5-72b-instruct-awq \
  --port 8000 \
  --quantization awq_marlin \
  --dtype float16 \
  --gpu-memory-utilization 0.85 \
  --max-model-len 32768 \
  --max-num-seqs 256 \
  --enable-prefix-caching \
  --enable-auto-tool-choice \
  --tool-call-parser hermes \
  --trust-remote-code
Restart=on-failure
RestartSec=30
StandardOutput=journal
StandardError=journal
SyslogIdentifier=vllm

[Install]
WantedBy=multi-user.target
