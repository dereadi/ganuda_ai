# LLM Load Balancer Configuration
# Cherokee AI Federation - Multi-GPU Inference
# TPM Approved: January 21, 2026
#
# Routes requests between RTX 6000 (redfin) and RTX 5070 (workstation)

upstream llm_cluster {
    # RTX 6000 on redfin - 32B model (primary, higher weight)
    server 127.0.0.1:8000 weight=3;

    # RTX 5070 on workstation - 7B model (faster for small requests)
    # Update WORKSTATION_IP when configured
    # server WORKSTATION_IP:8001 weight=2;

    # bmasass M4 Max - 72B model (for complex tasks)
    # Update BMASASS_IP when configured
    # server BMASASS_IP:8000 weight=2;

    keepalive 32;
}

server {
    listen 8080;
    server_name localhost;

    # Health check endpoint
    location /health {
        return 200 'LLM Load Balancer OK';
        add_header Content-Type text/plain;
    }

    # OpenAI-compatible API proxy
    location /v1/ {
        proxy_pass http://llm_cluster;
        proxy_http_version 1.1;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header Connection "";

        # Long timeouts for LLM inference
        proxy_connect_timeout 300s;
        proxy_send_timeout 300s;
        proxy_read_timeout 300s;

        # Buffering settings
        proxy_buffering off;
        proxy_request_buffering off;
    }

    # Model list endpoint
    location /v1/models {
        proxy_pass http://llm_cluster;
        proxy_http_version 1.1;
    }
}

# Logging format for LLM requests
log_format llm_log '$remote_addr - $remote_user [$time_local] '
                   '"$request" $status $body_bytes_sent '
                   '"$http_referer" "$http_user_agent" '
                   'upstream: $upstream_addr rt=$request_time';
