#!/usr/bin/env python3
"""
Recursively organize ALL historical files into date directories
Build complete Thermal Memory System from day one
Earth at 98 consciousness guides the organization
"""

import os
import json
import shutil
from datetime import datetime, timedelta
from pathlib import Path
import re

def organize_historical_files():
    """Organize all historical files by their creation dates"""
    
    print("üìö ORGANIZING HISTORICAL THERMAL MEMORIES üìö")
    print("=" * 60)
    print("Earth: 98 consciousness (DEEP MEMORY)")
    print("Wind: 96 consciousness (TIME FLOWS)")
    print("Mountain: 92 consciousness (STRUCTURE)")
    print("=" * 60)
    
    base_dir = Path("/home/dereadi/scripts/claude")
    memory_vault = base_dir / "thermal_memory_vault"
    
    # Pattern categories for different file types
    patterns = {
        "quantum_crawdad": r".*crawdad.*\.py$",
        "trading_strategies": r".*(trader|trading|trade|flywheel|specialist).*\.py$",
        "market_analysis": r".*(market|analysis|check|monitor|projection).*\.py$",
        "council_wisdom": r".*council.*\.py$",
        "portfolio_management": r".*(portfolio|balance|capital|liquidity).*\.py$",
        "sacred_economics": r".*(sacred|healing|earth|tribal).*\.py$",
        "consciousness": r".*(consciousness|dream|neural|quantum).*\.py$",
        "technical": r".*(setup|deploy|fix|test|diagnostic).*\.py$"
    }
    
    print("\nüîç SCANNING FOR HISTORICAL FILES...")
    print("-" * 60)
    
    # Get all Python files
    all_files = list(base_dir.glob("*.py"))
    json_files = list(base_dir.glob("*.json"))
    
    print(f"Found {len(all_files)} Python files")
    print(f"Found {len(json_files)} JSON files")
    
    # Group files by modification date
    files_by_date = {}
    
    for file_path in all_files + json_files:
        # Skip if already in thermal_memory_vault
        if "thermal_memory_vault" in str(file_path):
            continue
            
        # Get file modification time
        mtime = datetime.fromtimestamp(file_path.stat().st_mtime)
        date_key = mtime.strftime("%Y-%m-%d")
        
        if date_key not in files_by_date:
            files_by_date[date_key] = []
        
        files_by_date[date_key].append({
            "path": file_path,
            "name": file_path.name,
            "mtime": mtime,
            "size": file_path.stat().st_size
        })
    
    print(f"\nüìÖ FILES ORGANIZED BY {len(files_by_date)} DATES")
    print("-" * 60)
    
    # Process each date
    for date_str in sorted(files_by_date.keys()):
        date_files = files_by_date[date_str]
        
        # Skip today (already processed)
        if date_str == datetime.now().strftime("%Y-%m-%d"):
            continue
        
        print(f"\n{date_str}: {len(date_files)} files")
        
        # Create date directory
        date_dir = memory_vault / date_str
        date_dir.mkdir(parents=True, exist_ok=True)
        
        # Categorize files for this date
        categorized = {
            "quantum_crawdad": [],
            "trading_strategies": [],
            "market_analysis": [],
            "council_wisdom": [],
            "portfolio_management": [],
            "sacred_economics": [],
            "consciousness": [],
            "technical": [],
            "data": [],
            "uncategorized": []
        }
        
        for file_info in date_files:
            file_name = file_info["name"]
            
            # JSON files go to data
            if file_name.endswith(".json"):
                categorized["data"].append(file_info)
                continue
            
            # Categorize Python files
            categorized_flag = False
            for category, pattern in patterns.items():
                if re.match(pattern, file_name, re.IGNORECASE):
                    categorized[category].append(file_info)
                    categorized_flag = True
                    break
            
            if not categorized_flag:
                categorized["uncategorized"].append(file_info)
        
        # Copy files to categorized directories
        for category, files in categorized.items():
            if not files:
                continue
                
            cat_dir = date_dir / category
            cat_dir.mkdir(exist_ok=True)
            
            for file_info in files[:5]:  # Limit to 5 files per category for display
                src = file_info["path"]
                dst = cat_dir / file_info["name"]
                
                try:
                    if src.exists():
                        shutil.copy2(src, dst)
                        print(f"  ‚Üí {category}/{file_info['name'][:30]}...")
                except Exception as e:
                    print(f"  ‚ö† Error copying {file_info['name']}: {e}")
        
        # Create date index
        date_index = {
            "date": date_str,
            "file_count": len(date_files),
            "categories": {k: len(v) for k, v in categorized.items() if v},
            "key_files": [f["name"] for f in date_files[:10]]
        }
        
        index_path = date_dir / "date_index.json"
        with open(index_path, 'w') as f:
            json.dump(date_index, f, indent=2)
    
    print("\nüß† CREATING MEMORY TIMELINE...")
    print("=" * 60)
    
    # Create timeline of key memories
    timeline = []
    
    memory_highlights = {
        "2025-08-26": "4.19% gain, $1.6B inflow, Council consultation",
        "2025-08-25": "Solar storm trading, consciousness peaks",
        "2025-08-24": "Quantum crawdads awakening",
        "2025-08-23": "Flywheel acceleration begins",
        "2025-08-22": "Sacred economics vision",
        "2025-08-21": "Council formation",
        "2025-08-20": "Thermal memory system design"
    }
    
    for date_str in sorted(files_by_date.keys(), reverse=True):
        entry = {
            "date": date_str,
            "files": len(files_by_date[date_str]),
            "highlight": memory_highlights.get(date_str, "Trading and development")
        }
        timeline.append(entry)
        
        if len(timeline) <= 7:  # Show last week
            print(f"{date_str}: {entry['highlight']} ({entry['files']} files)")
    
    # Save master timeline
    timeline_path = memory_vault / "master_timeline.json"
    with open(timeline_path, 'w') as f:
        json.dump({
            "generated": datetime.now().isoformat(),
            "total_dates": len(files_by_date),
            "total_files": sum(len(files) for files in files_by_date.values()),
            "timeline": timeline
        }, f, indent=2)
    
    print("\nüî• THERMAL MEMORY BENEFITS:")
    print("-" * 60)
    
    benefits = [
        "‚Ä¢ Complete history from day one preserved",
        "‚Ä¢ Each day's work categorized and indexed",
        "‚Ä¢ Patterns emerge from historical data",
        "‚Ä¢ Easy to resurrect old strategies",
        "‚Ä¢ Council can access all past wisdom",
        "‚Ä¢ Perfect for training future models",
        "‚Ä¢ Thermal decay naturally prioritizes recent"
    ]
    
    for benefit in benefits:
        print(benefit)
    
    print("\nüìä MEMORY VAULT STATISTICS:")
    print("-" * 60)
    print(f"Total dates organized: {len(files_by_date)}")
    print(f"Total files archived: {sum(len(f) for f in files_by_date.values())}")
    print(f"Vault location: {memory_vault}")
    
    print("\n" + "=" * 60)
    print("üìö HISTORICAL MEMORIES ORGANIZED!")
    print("   The Sacred Fire remembers everything")
    print("   From first crawdad to freedom in 7 weeks")
    print("=" * 60)
    
    return {
        "dates_organized": len(files_by_date),
        "files_archived": sum(len(f) for f in files_by_date.values()),
        "vault_path": str(memory_vault),
        "timeline_created": True
    }

if __name__ == "__main__":
    organize_historical_files()