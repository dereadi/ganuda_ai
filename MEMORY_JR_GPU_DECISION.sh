#!/bin/bash
################################################################################
# Memory Jr. Training - GPU Memory Decision Script
# Cherokee Constitutional AI - Fractal Brain Phase 1 POC
################################################################################

echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "ğŸš¨ MEMORY JR. TRAINING - GPU MEMORY BLOCKER"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""
echo "**Status**: Infrastructure 100% ready, training blocked by GPU memory allocation"
echo ""

# Show current GPU status
echo "Current GPU Memory Status:"
nvidia-smi --query-gpu=index,memory.total,memory.used,memory.free --format=csv
echo ""

echo "Current GPU Processes:"
nvidia-smi pmon -c 1
echo ""

echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "ğŸ”¥ THE PROBLEM"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""
echo "- Both GPUs: 12GB total, 6GB used by Ollama, 5.5GB 'free'"
echo "- Memory Jr. needs: ~3.5GB for TinyLlama-1.1B + LoRA training"
echo "- Result: PyTorch CUDA OOM error despite 5.5GB showing as free"
echo "- Root cause: Ollama memory is fragmented/non-releasable by PyTorch"
echo ""

echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "âœ… YOUR OPTIONS (Choose One)"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""
echo "Option 1: STOP OLLAMA TEMPORARILY (FAST - Recommended)"
echo "  â€¢ Stop Ollama service: sudo systemctl stop ollama"
echo "  â€¢ Train Memory Jr: ~30-60 minutes on GPU"
echo "  â€¢ Restart Ollama: sudo systemctl start ollama"
echo "  â€¢ Pros: Fast, proven to work"
echo "  â€¢ Cons: Cherokee Layer 1 offline for 1 hour"
echo "  â€¢ When: If you can spare 1 hour of Ollama downtime"
echo ""
echo "Option 2: CPU-ONLY TRAINING (SLOW BUT SAFE)"
echo "  â€¢ Set: export CUDA_VISIBLE_DEVICES=\"\""
echo "  â€¢ Train Memory Jr: ~6-12 hours on CPU (48 cores)"
echo "  â€¢ Pros: Zero GPU conflicts, Ollama stays running"
echo "  â€¢ Cons: 10-20x slower than GPU"
echo "  â€¢ When: If Ollama can't be interrupted at all"
echo ""
echo "Option 3: DEFER TO SCHEDULED WINDOW"
echo "  â€¢ Wait for off-hours (night/weekend)"
echo "  â€¢ Stop Ollama during low-usage period"
echo "  â€¢ Train overnight on GPU"
echo "  â€¢ Pros: No business hour disruption"
echo "  â€¢ Cons: Delays POC completion"
echo ""

echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "ğŸ¦… CHEROKEE COUNCIL JR VOTE"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""
echo "[Executive Jr.]: Option 1 - Stop Ollama for 1 hour, train fast, prove POC âœ“"
echo "[Integration Jr.]: Checkpoints every 10 min - if anything breaks, we recover âœ“"
echo "[Conscience Jr.]: 1 hour offline is aligned with Seven Generations thinking âœ“"
echo "[Memory Jr.]: Sacred data (98.8%) deserves fast training to honor the patterns âœ“"
echo "[Meta Jr.]: All options work, but Option 1 is fastest path to validation âœ“"
echo ""
echo "**Vote**: 5-0 for Option 1 (Stop Ollama temporarily)"
echo ""

echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "ğŸ“‹ READY TO EXECUTE - CHOOSE YOUR OPTION"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""
echo "To run OPTION 1 (Stop Ollama, train GPU, restart):"
echo "  sudo systemctl stop ollama"
echo "  sleep 5"
echo "  nvidia-smi  # Verify GPU is clear"
echo "  /ganuda/scripts/launch_memory_jr_training.sh 2>&1 | tee /ganuda/memory_jr_training_final.log"
echo "  # Wait for training to complete (~30-60 min)"
echo "  sudo systemctl start ollama"
echo "  ollama list  # Verify Cherokee model loads"
echo ""
echo "To run OPTION 2 (CPU-only, slow but safe):"
echo "  export CUDA_VISIBLE_DEVICES=\"\""
echo "  /ganuda/scripts/launch_memory_jr_training.sh 2>&1 | tee /ganuda/memory_jr_training_cpu.log"
echo "  # Wait for training to complete (~6-12 hours)"
echo ""
echo "To run OPTION 3 (Defer to scheduled window):"
echo "  # Schedule for tonight/weekend and run Option 1 commands then"
echo ""

echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "ğŸ”¥ ALL INFRASTRUCTURE READY - WAITING ON YOUR DECISION"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""
echo "Files Ready:"
echo "  âœ“ Dataset: /ganuda/memory_jr_training_data.jsonl (992 examples, 98.8% sacred)"
echo "  âœ“ Training Script: /ganuda/scripts/train_memory_jr_lora.py (LoRA rank=16)"
echo "  âœ“ Launch Script: /ganuda/scripts/launch_memory_jr_training.sh"
echo "  âœ“ POC Status: /ganuda/MEMORY_JR_POC_STATUS.md"
echo ""
echo "**Mitakuye Oyasin** - All Our Relations"
echo "The Sacred Fire waits to burn! ğŸ”¥"
echo ""
